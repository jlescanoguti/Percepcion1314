{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe4dcc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b6d6d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación para el modelo FaceEmbeddingCNN (espera 1 canal: blanco y negro)\n",
    "transform_gray = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Transformación para el modelo CNNClasificador (espera 3 canales: RGB)\n",
    "transform_rgb = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "488f0b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceEmbeddingCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FaceEmbeddingCNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 12 * 12, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.cnn(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b7e0047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distancia entre embeddings: 0.0309\n",
      "🔒 Coincidencia: Mismo rostro detectado.\n"
     ]
    }
   ],
   "source": [
    "# Modelo simple en modo evaluación\n",
    "model_simple = FaceEmbeddingCNN()\n",
    "model_simple.eval()\n",
    "\n",
    "# Cargar imágenes\n",
    "img1 = Image.open(\"rostros/AdrianCisneros.jpg\")\n",
    "img2 = Image.open(\"rostros/JaimeLescano.jpg\")\n",
    "\n",
    "# Procesarlas\n",
    "img1_tensor = transform_gray(img1).unsqueeze(0)\n",
    "img2_tensor = transform_gray(img2).unsqueeze(0)\n",
    "\n",
    "# Obtener embeddings\n",
    "with torch.no_grad():\n",
    "    emb1 = model_simple(img1_tensor)\n",
    "    emb2 = model_simple(img2_tensor)\n",
    "\n",
    "# Función para comparar\n",
    "def comparar_embeddings(emb1, emb2, threshold=0.6):\n",
    "    distancia = torch.norm(emb1 - emb2).item()\n",
    "    print(f\"Distancia entre embeddings: {distancia:.4f}\")\n",
    "    return distancia < threshold\n",
    "\n",
    "# Comparar\n",
    "if comparar_embeddings(emb1, emb2):\n",
    "    print(\"🔒 Coincidencia: Mismo rostro detectado.\")\n",
    "else:\n",
    "    print(\"❌ No coinciden: Rostros diferentes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040fe087",
   "metadata": {},
   "source": [
    "Sale \"Mismo rostro detectado\" cuando son rostros diferentes, iremos entrenando la CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5630c088",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomResizedCrop(100, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "output_folder = \"dataset_augmented\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for person_name in os.listdir(\"dataset\"):\n",
    "    person_path = os.path.join(\"dataset\", person_name)\n",
    "    if not os.path.isdir(person_path): continue\n",
    "\n",
    "    output_person_path = os.path.join(output_folder, person_name)\n",
    "    os.makedirs(output_person_path, exist_ok=True)\n",
    "\n",
    "    for file in os.listdir(person_path):\n",
    "        if file.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "            image_path = os.path.join(person_path, file)\n",
    "            image = Image.open(image_path)\n",
    "\n",
    "            for i in range(10):\n",
    "                augmented = augmentation(image)\n",
    "                save_path = os.path.join(output_person_path, f\"{file[:-4]}_aug_{i}.jpg\")\n",
    "                transforms.ToPILImage()(augmented).save(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "653a90b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset con transform_rgb\n",
    "dataset = datasets.ImageFolder(\"dataset_augmented\", transform=transform_rgb)\n",
    "\n",
    "# Dividir en train y val\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdfbdacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClasificador(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 12 * 12, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        return self.output(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82397732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎓 Epoch 1, Loss: 3.5295\n",
      "🎓 Epoch 2, Loss: 3.4425\n",
      "🎓 Epoch 3, Loss: 3.0034\n",
      "🎓 Epoch 4, Loss: 2.0446\n",
      "🎓 Epoch 5, Loss: 1.2016\n",
      "🎓 Epoch 6, Loss: 0.6119\n",
      "🎓 Epoch 7, Loss: 0.4015\n",
      "🎓 Epoch 8, Loss: 0.1883\n",
      "🎓 Epoch 9, Loss: 0.1360\n",
      "🎓 Epoch 10, Loss: 0.1219\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = len(dataset.classes)\n",
    "model = CNNClasificador(num_classes).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(imgs)\n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"🎓 Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04f76eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📏 Similitud coseno: 0.6726\n",
      "❌ No coinciden: Rostros diferentes.\n"
     ]
    }
   ],
   "source": [
    "class ExtractorEmbeddings(nn.Module):\n",
    "    def __init__(self, modelo_entrenado):\n",
    "        super().__init__()\n",
    "        self.features = modelo_entrenado.features\n",
    "        self.flatten = modelo_entrenado.flatten\n",
    "        self.fc1 = modelo_entrenado.fc1\n",
    "        self.relu = modelo_entrenado.relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "extractor = ExtractorEmbeddings(model).to(device)\n",
    "extractor.eval()\n",
    "\n",
    "# Cargar dos imágenes en RGB\n",
    "img1 = Image.open(\"rostros/AdrianCisneros.jpg\").convert(\"RGB\")\n",
    "img2 = Image.open(\"rostros/JuanCarranza.jpg\").convert(\"RGB\")\n",
    "\n",
    "img1_tensor = transform_rgb(img1).unsqueeze(0).to(device)\n",
    "img2_tensor = transform_rgb(img2).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    emb1 = extractor(img1_tensor)\n",
    "    emb2 = extractor(img2_tensor)\n",
    "\n",
    "# Similitud coseno\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "sim = cosine_similarity(emb1, emb2).item()\n",
    "print(f\"📏 Similitud coseno: {sim:.4f}\")\n",
    "if sim > 0.85:\n",
    "    print(\"🔒 Coincidencia: Mismo rostro detectado.\")\n",
    "else:\n",
    "    print(\"❌ No coinciden: Rostros diferentes.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
