{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe4dcc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b6d6d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaci√≥n para el modelo FaceEmbeddingCNN (espera 1 canal: blanco y negro)\n",
    "transform_gray = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Transformaci√≥n para el modelo CNNClasificador (espera 3 canales: RGB)\n",
    "transform_rgb = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "488f0b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceEmbeddingCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FaceEmbeddingCNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 12 * 12, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.cnn(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b7e0047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distancia entre embeddings: 0.0309\n",
      "üîí Coincidencia: Mismo rostro detectado.\n"
     ]
    }
   ],
   "source": [
    "# Modelo simple en modo evaluaci√≥n\n",
    "model_simple = FaceEmbeddingCNN()\n",
    "model_simple.eval()\n",
    "\n",
    "# Cargar im√°genes\n",
    "img1 = Image.open(\"rostros/AdrianCisneros.jpg\")\n",
    "img2 = Image.open(\"rostros/JaimeLescano.jpg\")\n",
    "\n",
    "# Procesarlas\n",
    "img1_tensor = transform_gray(img1).unsqueeze(0)\n",
    "img2_tensor = transform_gray(img2).unsqueeze(0)\n",
    "\n",
    "# Obtener embeddings\n",
    "with torch.no_grad():\n",
    "    emb1 = model_simple(img1_tensor)\n",
    "    emb2 = model_simple(img2_tensor)\n",
    "\n",
    "# Funci√≥n para comparar\n",
    "def comparar_embeddings(emb1, emb2, threshold=0.6):\n",
    "    distancia = torch.norm(emb1 - emb2).item()\n",
    "    print(f\"Distancia entre embeddings: {distancia:.4f}\")\n",
    "    return distancia < threshold\n",
    "\n",
    "# Comparar\n",
    "if comparar_embeddings(emb1, emb2):\n",
    "    print(\"üîí Coincidencia: Mismo rostro detectado.\")\n",
    "else:\n",
    "    print(\"‚ùå No coinciden: Rostros diferentes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040fe087",
   "metadata": {},
   "source": [
    "Sale \"Mismo rostro detectado\" cuando son rostros diferentes, iremos entrenando la CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5630c088",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomResizedCrop(100, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "output_folder = \"dataset_augmented\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for person_name in os.listdir(\"dataset\"):\n",
    "    person_path = os.path.join(\"dataset\", person_name)\n",
    "    if not os.path.isdir(person_path): continue\n",
    "\n",
    "    output_person_path = os.path.join(output_folder, person_name)\n",
    "    os.makedirs(output_person_path, exist_ok=True)\n",
    "\n",
    "    for file in os.listdir(person_path):\n",
    "        if file.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "            image_path = os.path.join(person_path, file)\n",
    "            image = Image.open(image_path)\n",
    "\n",
    "            for i in range(10):\n",
    "                augmented = augmentation(image)\n",
    "                save_path = os.path.join(output_person_path, f\"{file[:-4]}_aug_{i}.jpg\")\n",
    "                transforms.ToPILImage()(augmented).save(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "653a90b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset con transform_rgb\n",
    "dataset = datasets.ImageFolder(\"dataset_augmented\", transform=transform_rgb)\n",
    "\n",
    "# Dividir en train y val\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdfbdacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClasificador(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 12 * 12, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        return self.output(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82397732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéì Epoch 1, Loss: 3.5295\n",
      "üéì Epoch 2, Loss: 3.4425\n",
      "üéì Epoch 3, Loss: 3.0034\n",
      "üéì Epoch 4, Loss: 2.0446\n",
      "üéì Epoch 5, Loss: 1.2016\n",
      "üéì Epoch 6, Loss: 0.6119\n",
      "üéì Epoch 7, Loss: 0.4015\n",
      "üéì Epoch 8, Loss: 0.1883\n",
      "üéì Epoch 9, Loss: 0.1360\n",
      "üéì Epoch 10, Loss: 0.1219\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = len(dataset.classes)\n",
    "model = CNNClasificador(num_classes).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(imgs)\n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"üéì Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04f76eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìè Similitud coseno: 0.6726\n",
      "‚ùå No coinciden: Rostros diferentes.\n"
     ]
    }
   ],
   "source": [
    "class ExtractorEmbeddings(nn.Module):\n",
    "    def __init__(self, modelo_entrenado):\n",
    "        super().__init__()\n",
    "        self.features = modelo_entrenado.features\n",
    "        self.flatten = modelo_entrenado.flatten\n",
    "        self.fc1 = modelo_entrenado.fc1\n",
    "        self.relu = modelo_entrenado.relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "extractor = ExtractorEmbeddings(model).to(device)\n",
    "extractor.eval()\n",
    "\n",
    "# Cargar dos im√°genes en RGB\n",
    "img1 = Image.open(\"rostros/AdrianCisneros.jpg\").convert(\"RGB\")\n",
    "img2 = Image.open(\"rostros/JuanCarranza.jpg\").convert(\"RGB\")\n",
    "\n",
    "img1_tensor = transform_rgb(img1).unsqueeze(0).to(device)\n",
    "img2_tensor = transform_rgb(img2).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    emb1 = extractor(img1_tensor)\n",
    "    emb2 = extractor(img2_tensor)\n",
    "\n",
    "# Similitud coseno\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "sim = cosine_similarity(emb1, emb2).item()\n",
    "print(f\"üìè Similitud coseno: {sim:.4f}\")\n",
    "if sim > 0.85:\n",
    "    print(\"üîí Coincidencia: Mismo rostro detectado.\")\n",
    "else:\n",
    "    print(\"‚ùå No coinciden: Rostros diferentes.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
